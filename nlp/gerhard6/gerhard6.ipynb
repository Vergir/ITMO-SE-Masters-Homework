{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gerhard6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnVMJ32mwref",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9dbabb45-0ac5-41ad-ea62-b72b6bc4c701"
      },
      "source": [
        "import flair\n",
        "import pickle\n",
        "import sys\n",
        "import sklearn\n",
        "import sklearn.cluster\n",
        "import scipy\n",
        "import scipy.cluster\n",
        "import matplotlib\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pyclustering\n",
        "\n",
        "TEXT_FILENAME = '1056.txt'\n",
        "EMBEDDINGS_FILENAME = '1056.model'\n",
        "\n",
        "def clustering_filename(radius, neighbors):\n",
        "  return f'1056_radius{radius}_neighbors{neighbors}.optics'\n",
        "\n",
        "def save_clustering(radius, neighbors, clustering):\n",
        "  with open(clustering_filename(radius, neighbors), 'wb') as clustering_file:\n",
        "    data = (clustering.get_clusters(), clustering.get_noise(), clustering.get_ordering())\n",
        "    pickle.dump(data, clustering_file)\n",
        "  \n",
        "def load_clustering(radius, neighbors):  \n",
        "  with open(clustering_filename(radius, neighbors), 'rb') as clustering_file:\n",
        "    data = pickle.load(clustering_file)\n",
        "  \n",
        "  return data[0], data[1], data[2]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCgbL9V3xOPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "19fe7605-1130-4d1b-c42a-07e01306ff7d"
      },
      "source": [
        "with open(TEXT_FILENAME, 'r') as text_file:\n",
        "  corpus = text_file.read()\n",
        "\n",
        "sentences = nltk.tokenize.sent_tokenize(corpus)\n",
        "\n",
        "glove_embedding = flair.embeddings.WordEmbeddings('glove')\n",
        "flair_embedding_forward = flair.embeddings.FlairEmbeddings('news-forward')\n",
        "flair_embedding_backward = flair.embeddings.FlairEmbeddings('news-backward')\n",
        "document_embeddings = flair.embeddings.DocumentPoolEmbeddings([glove_embedding, flair_embedding_backward, flair_embedding_forward])\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "count = len(sentences)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  flair_sentence = flair.data.Sentence(sentence)\n",
        "  document_embeddings.embed(flair_sentence)\n",
        "  embeddings.append(scipy.cluster.vq.whiten(flair_sentence.get_embedding()))\n",
        "\n",
        "with open(EMBEDDINGS_FILENAME, 'wb') as file:\n",
        "  pickle.dump(embeddings, file)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:20:21,960 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpik2bq0um\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 160000128/160000128 [00:17<00:00, 9057088.16B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:20:40,808 copying /tmp/tmpik2bq0um to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:20:41,086 removing temp file /tmp/tmpik2bq0um\n",
            "2019-05-15 07:20:42,196 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmpw1pz9wn0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21494764/21494764 [00:03<00:00, 5478402.01B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:20:47,263 copying /tmp/tmpw1pz9wn0 to cache at /root/.flair/embeddings/glove.gensim\n",
            "2019-05-15 07:20:47,295 removing temp file /tmp/tmpw1pz9wn0\n",
            "2019-05-15 07:20:47,297 this function is deprecated, use smart_open.open instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:20:49,884 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpz5w4lbcj\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034624/73034624 [00:09<00:00, 7576548.83B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:21:00,694 copying /tmp/tmpz5w4lbcj to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n",
            "2019-05-15 07:21:00,789 removing temp file /tmp/tmpz5w4lbcj\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:21:09,010 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp_th6_a0g\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034575/73034575 [00:07<00:00, 9132416.69B/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:21:18,235 copying /tmp/tmp_th6_a0g to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-15 07:21:18,338 removing temp file /tmp/tmp_th6_a0g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoa5GDVG6k3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(EMBEDDINGS_FILENAME, 'rb') as embeddings_file:\n",
        "  embeddings = pickle.load(embeddings_file)\n",
        "  \n",
        "pyclustering_formatted_data = []\n",
        "for sentence in embeddings:\n",
        "  pyclustering_formatted_data.append(sentence.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPRVvaFV6oHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "radius = 0.5\n",
        "neighbors = 3\n",
        "optics_instance = pyclustering.cluster.optics.optics(pyclustering_formatted_data, radius, neighbors)\n",
        "\n",
        "optics_instance.process()\n",
        "\n",
        "save_clustering(radius, neighbors, optics_instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB8tm2w-BGnx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5583263d-a327-4335-f051-46a86972a68e"
      },
      "source": [
        "clusters, noise, ordering = load_clustering(radius, neighbors)\n",
        "\n",
        "print(len(clusters))\n",
        "print(len(noise))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "9176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0zhO4PnJshD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "593d9d04-a29d-403f-ff5d-4e14667ed60e"
      },
      "source": [
        "for cluster in clusters:\n",
        "  print(len(cluster))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "5\n",
            "5\n",
            "4\n",
            "11\n",
            "5\n",
            "4\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSYYXPb_GCFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1111
        },
        "outputId": "3c33aeea-9882-434e-ffcc-930a4fef9950"
      },
      "source": [
        "with open(TEXT_FILENAME, 'r') as text_file:\n",
        "  corpus = text_file.read()\n",
        "  \n",
        "sentences = nltk.tokenize.sent_tokenize(corpus)\n",
        "\n",
        "for i, cluster in enumerate(clusters):\n",
        "  print(f'cluster {i}')\n",
        "  for sentence_index in cluster:\n",
        "    print(sentences[sentence_index])\n",
        "  print()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 0\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "\n",
            "cluster 1\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "\n",
            "cluster 2\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "\n",
            "cluster 3\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "\n",
            "cluster 4\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "\n",
            "cluster 5\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "\n",
            "cluster 6\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "\n",
            "cluster 7\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4ziCICPHwNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1302
        },
        "outputId": "66591a96-c6cc-4dbe-c3af-3760dce0148c"
      },
      "source": [
        "for i, cluster in enumerate(clusters):\n",
        "  print(f'cluster {i}')\n",
        "  for sentence_index in cluster:\n",
        "    print(sentences[sentence_index+1])\n",
        "  print()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 0\n",
            ".\n",
            "\"There is no use trying to see me,\" she said toward the last.\n",
            ".\n",
            "It was\n",
            "soft because she had never used it to work with.\n",
            ".\n",
            ". \"\n",
            "I guess that's all.\"\n",
            ".\n",
            "yes, he had written other\n",
            "books; well, he would go to the free library the first thing in the\n",
            "morning and try to get hold of some of Swinburne's stuff.\n",
            "\n",
            "cluster 1\n",
            "Mrs. Higginbotham made no reply.\n",
            "\"It is.\"\n",
            "\"It came to me suddenly.\"\n",
            "Or were they afraid of life, these writers and\n",
            "editors and readers?\n",
            "\"You never laid eyes on me before.\"\n",
            "\n",
            "cluster 2\n",
            "He was oppressed by the utter squalidness of it\n",
            "all.\n",
            "\"That night was the one night for\n",
            "me.\n",
            "\"Why, the papers were full of it.\n",
            "\"Never mind.\n",
            "\"She's my lady friend,\" Jim explained, \"and she's a peach.\n",
            "\n",
            "cluster 3\n",
            "\"It ain't Bill at all,\" the other broke in.\n",
            "\"No, for less than a week's work.\n",
            "\"All of them--the whole kit and crew.\"\n",
            "\"That sounds wrong,\" he said slowly.\n",
            "\n",
            "cluster 4\n",
            "\"How'd yeh know?\"\n",
            "\"Is it a chill?\n",
            "Martin shook his head.\n",
            "She shook her head.\n",
            "\"I do!\n",
            "\"Yes.\n",
            "\"I had hoped and planned otherwise.\n",
            "Martin nodded, and went on arranging the books on a kitchen table which\n",
            "served in the room in place of a wash-stand.\n",
            "\"It was confused,\" she answered.\n",
            "\"And don't forget that I feel in me\n",
            "this capacity to write--I can't explain it; I just know that it is in\n",
            "me.\"\n",
            "\"Oh, nothin',\" the dark girl answered, with a toss of her head.\n",
            "\n",
            "cluster 5\n",
            "\"My father and mother are dead.\n",
            "\"That social-settlement woman is no more than a sociological poll-parrot.\n",
            "Eating was farthest from his desire, and he wondered\n",
            "that he should ever have been hungry in his life.\n",
            "\"I meant that I was going to work at my writing.\"\n",
            "\"Beauty has significance, but I never knew its significance before.\n",
            "\n",
            "cluster 6\n",
            "\"Rest.\n",
            "Brissenden considered judicially, and nodded his head.\n",
            "\"His rooms we're going to.\n",
            "\"Men, intelligent men, and not the gibbering nonentities I found you\n",
            "consorting with in that trader's den.\n",
            "\n",
            "cluster 7\n",
            "\"Don't lie,\" he commanded, and the nod of her head affirmed his charge.\n",
            "\"Is all this of your own free will?\"\n",
            "\"Nobody has faith in me, Gertrude, except myself.\"\n",
            "\"No, it is so different from anything I have read.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gs9vSCZId3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "radius = 10\n",
        "neighbors = 3\n",
        "optics_instance = pyclustering.cluster.optics.optics(pyclustering_formatted_data, radius, neighbors)\n",
        "# Performs cluster analysis.\n",
        "optics_instance.process()\n",
        "\n",
        "save_clustering(radius, neighbors, optics_instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbeZuhhgJ2Lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5ebf42c8-289e-4e78-bbb9-4dccbe69f066"
      },
      "source": [
        "clusters, noise, ordering = load_clustering(radius, neighbors)\n",
        "\n",
        "print(len(clusters))\n",
        "print(len(noise))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "9164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egFHorNNoFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1389
        },
        "outputId": "2b7d7c10-21f9-4387-9991-590596facdff"
      },
      "source": [
        "with open(TEXT_FILENAME, 'r') as text_file:\n",
        "  corpus = text_file.read()\n",
        "  \n",
        "sentences = nltk.tokenize.sent_tokenize(corpus)\n",
        "\n",
        "for i, cluster in enumerate(clusters):\n",
        "  print(f'cluster {i}')\n",
        "  for sentence_index in cluster:\n",
        "    print(sentences[sentence_index])\n",
        "  print()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 0\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "\n",
            "cluster 1\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "\n",
            "cluster 2\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "\n",
            "cluster 3\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "\n",
            "cluster 4\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "\n",
            "cluster 5\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "\n",
            "cluster 6\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "\n",
            "cluster 7\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "\n",
            "cluster 8\n",
            "1.E.3.\n",
            "1.E.4.\n",
            "1.E.6.\n",
            "1.E.7.\n",
            "1.E.8.\n",
            "1.E.5.\n",
            "1.E.9.\n",
            "\n",
            "cluster 9\n",
            "1.F.2.\n",
            "1.F.3.\n",
            "1.F.4.\n",
            "1.F.6.\n",
            "1.F.5.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gBGfB6MN4TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "radius = 25\n",
        "neighbors = 3\n",
        "optics_instance = pyclustering.cluster.optics.optics(pyclustering_formatted_data, radius, neighbors)\n",
        "# Performs cluster analysis.\n",
        "optics_instance.process()\n",
        "\n",
        "save_clustering(radius, neighbors, optics_instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRCoZlg-QCVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1b20d35-edac-4b51-b7e8-558e57b687b1"
      },
      "source": [
        "clusters, noise, ordering = load_clustering(radius, neighbors)\n",
        "\n",
        "print(len(clusters))\n",
        "print(len(noise))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24\n",
            "5695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjRdIT0SQIJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "c7e86b5e-64c9-43be-af61-ea5b15a87280"
      },
      "source": [
        "for cluster in clusters:\n",
        "  print(len(cluster))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3385\n",
            "9\n",
            "13\n",
            "5\n",
            "7\n",
            "12\n",
            "9\n",
            "12\n",
            "6\n",
            "7\n",
            "5\n",
            "8\n",
            "5\n",
            "5\n",
            "3\n",
            "4\n",
            "3\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "1\n",
            "7\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRbKgUj7QNwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4029
        },
        "outputId": "bc90b454-2371-4c3d-cf85-4d0b7b8fff39"
      },
      "source": [
        "with open(TEXT_FILENAME, 'r') as text_file:\n",
        "  corpus = text_file.read()\n",
        "  \n",
        "sentences = nltk.tokenize.sent_tokenize(corpus)\n",
        "\n",
        "for i, cluster in enumerate(clusters):\n",
        "  print(f'cluster {i}')\n",
        "  for sentence_index in cluster[:10]:\n",
        "    print(sentences[sentence_index])\n",
        "  print()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 0\n",
            "He wore rough clothes that smacked\n",
            "of the sea, and he was manifestly out of place in the spacious hall in\n",
            "which he found himself.\n",
            "It was the\n",
            "second time he had been out with her alone, and as they rode along\n",
            "through the balmy warmth, just chilled by she sea-breeze to refreshing\n",
            "coolness, he was profoundly impressed by the fact that it was a very\n",
            "beautiful and well-ordered world and that it was good to be alive and to\n",
            "love.\n",
            "He had no second-best\n",
            "suit that was presentable, and though he could go to the butcher and the\n",
            "baker, and even on occasion to his sister's, it was beyond all daring to\n",
            "dream of entering the Morse home so disreputably apparelled.\n",
            "He was stirred profoundly by the passing glimpse at the\n",
            "secret, and he was again caught up in the vision of sunlit spaces and\n",
            "starry voids--until it came to him that it was very quiet, and he saw\n",
            "Ruth regarding him with an amused expression and a smile in her eyes.\n",
            "It seemed to him that he had intruded\n",
            "upon the holy of holies, and slowly and carefully he moved his head aside\n",
            "from the contact which thrilled him like an electric shock and of which\n",
            "she had not been aware.\n",
            "It was the sublime abnegation of true love that comes\n",
            "to all lovers, and it came to him there, at the telephone, in a whirlwind\n",
            "of fire and glory; and to die for her, he felt, was to have lived and\n",
            "loved well.\n",
            "He was a fool to have ever\n",
            "left them, he thought; and he was very certain that his sum of happiness\n",
            "would have been greater had he remained with them and let alone the books\n",
            "and the people who sat in the high places.\n",
            "Never having been conscious of himself, he did not\n",
            "know that he had that in his being that drew love from women and which\n",
            "had been the cause of their reaching out for his youth.\n",
            "All that he was and was not, all that he had done and most of what he had\n",
            "not done, was spread out for the delectation of the public, accompanied\n",
            "by snapshots and photographs--the latter procured from the local\n",
            "photographer who had once taken Martin's picture and who promptly\n",
            "copyrighted it and put it on the market.\n",
            "She was aware only of the strength, and not of the medium,\n",
            "and when she seemed most carried away by what he had written, in reality\n",
            "she had been carried away by something quite foreign to it--by a thought,\n",
            "terrible and perilous, that had formed itself unsummoned in her brain.\n",
            "\n",
            "cluster 1\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "\n",
            "cluster 2\n",
            "It was merely food.\n",
            "It was simply a\n",
            "mistake.\n",
            "It was unmaidenly.\n",
            "It was baffling.\n",
            "It was accidental.\n",
            "It was perfect\n",
            "art.\n",
            "It was only natural.\n",
            "It was very white.\n",
            "It was a miracle.\n",
            "It was like a thunderbolt.\n",
            "\n",
            "cluster 3\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "he demanded.\n",
            "\n",
            "cluster 4\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head.\n",
            "Martin shook his head\n",
            "lugubriously.\n",
            "Martin shook his hand.\n",
            "\n",
            "cluster 5\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "she queried.\n",
            "she questioned.\n",
            "she quavered.\n",
            "she whispered.\n",
            "she sneered.\n",
            "she sobbed.\n",
            "she cried.\n",
            "\n",
            "cluster 6\n",
            "\"How do you know?\"\n",
            "\"How do you know?\"\n",
            "\"How do you mean?\"\n",
            "\"How do you mean?\"\n",
            "\"What do you mean?\"\n",
            "\"What do you want, Martin?\"\n",
            "Do you know?\"\n",
            "Do you know them?\"\n",
            "\"Yes, what do you want?\"\n",
            "\n",
            "cluster 7\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "he asked.\n",
            "\n",
            "cluster 8\n",
            "she asked earnestly.\n",
            "she asked suddenly.\n",
            "she asked warningly.\n",
            "she asked abruptly.\n",
            "he asked suddenly.\n",
            "he asked harshly.\n",
            "\n",
            "cluster 9\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head.\n",
            "He shook his head emphatically.\n",
            "He shook his head sadly.\n",
            "\n",
            "cluster 10\n",
            "How did he do it?\n",
            "How did the others do it?\n",
            "But how do you do it?\n",
            "But why do you love me?\n",
            "Where do you belong?\n",
            "\n",
            "cluster 11\n",
            "Martin challenged.\n",
            "Martin demanded.\n",
            "Martin demanded.\n",
            "Martin protested.\n",
            "Martin retorted.\n",
            "Martin shouted.\n",
            "Martin laughed.\n",
            "Martin declined.\n",
            "\n",
            "cluster 12\n",
            "That was it.\n",
            "That was it.\n",
            "That was it.\n",
            "That was all.\n",
            "And that was all.\n",
            "\n",
            "cluster 13\n",
            "There was no caprice, no chance.\n",
            "There was no check.\n",
            "There was no check.\n",
            "There was no time.\n",
            "There was no explaining it.\n",
            "\n",
            "cluster 14\n",
            "And so with Darwin.\n",
            "And so with him.\n",
            "And so with the\n",
            "editorial machine.\n",
            "\n",
            "cluster 15\n",
            "she asked.\n",
            "she asked.\n",
            "she asked.\n",
            "she asked\n",
            "abruptly.\n",
            "\n",
            "cluster 16\n",
            "he queried.\n",
            "he queried.\n",
            "he rejoined.\n",
            "\n",
            "cluster 17\n",
            "\"No,\" was the answer.\n",
            "\"Money,\" was the answer.\n",
            "\"No can,\" was the answer.\n",
            "\"Got to,\" was the answer.\n",
            "\n",
            "cluster 18\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "Martin asked.\n",
            "\n",
            "cluster 19\n",
            "Ruth was silent.\n",
            "Ruth was aghast.\n",
            "Ruth was overjoyed.\n",
            "Ruth was appalled.\n",
            "\n",
            "cluster 20\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "She shook her head.\n",
            "\n",
            "cluster 21\n",
            "It is\n",
            "brutal.\n",
            "\n",
            "cluster 22\n",
            "1.E.3.\n",
            "1.E.4.\n",
            "1.E.6.\n",
            "1.E.7.\n",
            "1.E.8.\n",
            "1.E.5.\n",
            "1.E.9.\n",
            "\n",
            "cluster 23\n",
            "1.F.1.\n",
            "1.F.2.\n",
            "1.F.3.\n",
            "1.F.4.\n",
            "1.F.6.\n",
            "1.F.5.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}